\title{Loom: High performance blockchain }

\author{
        Anatoly Yakovenko \\
        aeyakovenko@gmail.com\\
}
\date{}

\documentclass[12pt]{article}

\usepackage{graphicx}

\begin{document}
\maketitle

\begin{abstract}
A new Proof of History algorithm is proposed for global read consistency which can be used alongside a consensus algorithm to minimize messaging overhead in a Byzantine Fault Tolerant replicated state machine. It achieves performance by creating a single globally agreed upon order of events independent of network consensus. Nodes participating in the network only vote on a binary choice of accepting or rejecting the ordering. Without hardware failures, all the participating nodes are expected to agree with the proposed ordering with minimal communication overhead above the transaction data itself. Any consensus algorithm can be used, such as Proof of Work or Proof of Stake, a simple Proof of Stake consensus algorithm is proposed. To ensure high availability of data, an efficient streaming Proof of Replication is proposed which takes advantage of the time keeping properties provided by Proof of History.  The combination of PoRep and PoH provides a substantial defense against forgery of the ledger in terms of time and storage. The protocol is analyzed on a 1gbps network, and it is shown that throughput is limited by network or ECDSA digests, and with a GPU dedicated to ECDSA digests over \textbf{350k} and up to \textbf{700k} transactions per second with high availability is theoretically possible.

\end{abstract}

\section{Introduction}
This is time for all good men to come to the aid of their party!

%\paragraph{Outline}
%The remainder of this article is organized as follows.
%Section~\ref{previous work} gives account of previous work.
%Our new and exciting results are described in Section~\ref{results}.
%Finally, Section~\ref{conclusions} gives the conclusions.

\section{Design}\label{proof_of_history}

Loom is a Proof of History generator.  It takes a sequence of arbitrary user transactions.  It orders them in the most efficient way to process them, by maximizing memory throughput.  Executes the transactions on the current state that is stored in RAM.  Publishes the transactions and a signature of the state stored in RAM to the replications nodes.  Spools replicate the operations on their copies of the state, and publish their computed hash as confirmations via consensus algorithm.

\begin{figure}
  \begin{center}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/fig_4.png}
    \caption[Fig 4]{Transaction flow throught the network.\label{fig_1}}
  \end{center}
  \end{figure}

In the figure above, messages are sent by users into the Loom node, which orders them and broadcasts the order and the resulted computed state to the replicator nodes called Spools.  Spools then send a confirmation message of the computed state hash to the Loom.

Looms are elected by the network via its configured consensus algorithm, such as Proof of Stake or Proof of Work.  More on elections here.

Proof of History sequence guarantees global read consistency for the network that would require an attacker an investment of time to reverse.  A Proof of Stake algorithm is proposed for electing Looms and posting confirmations of the order.  A fast Proof of Replication is proposed for ledger and state availability, and as a defense against forgery attacks.
\section{Design}

\section{Proof of History}\label{proof_of_history}

Proof of History provides a way to cryptographically verify passage of time between two events. It uses a cryptographically secure function whose output cannot be predicted from the input, and must be completely executed to generate the output. The function is run in a sequence, it’s previous output as the current input, periodically recording the current output, and how many times it’s been called. The output can then be recomputed and verified by external computers in parallel by checking each period in parallel on a separate core. Data can be timestamped into this sequence by recording the data and the index it was mixed into the sequence. The timestamp then guarantees that the data was created sometime before this hash was generated in the sequence. Multiple generators can synchronize amongst each other by mixing their state into each others sequences. \\

\subsection{Description}

With a cryptographic function, like a cryptographic hash (sha256, md5,
sha-1), whose output cannot be predicted without running the function,
run the function from some random starting value and take its output
and pass it as the input into the same function again. And record the
number of times the function has been called and the output at each
call. \\\\
\noindent For example: \\\\\noindent
\texttt{
  sha256(\char`\"any random starting value\char`\") $\rightarrow$
  hash1, (n\_count~$=~1$) \\
  sha256(hash1) $\rightarrow$ hash2, (n\_count~$=~2$)\\
  sha256(hash2) $\rightarrow$ hash3, (n\_count~$=~3$)\\
}

\noindent Where \texttt{hashN} represents the actual hash output. \\

Instead of publishing every hash on every index, only a subset of
these hashes could be published at an interval.\\

\noindent For example:\\\\\noindent
\texttt{
 sha256(\char`\"any starting value\char`\") $\rightarrow$ hash1, (n\_count~$=1$)\\
\ldots\\
sha256(hash199) $\rightarrow$ hash200, (n\_count~$=200$)\\
\ldots\\
sha256(hash299) $\rightarrow$ hash300, (n\_count~$=300$)\\
}

This set of events can only be computed in sequence by a single computer thread, because there is no way to predict what the hash value at index $300$ is going to be without actually running the algorithm from the starting value $300$ times.

\begin{figure}
  \begin{center}
    \centering
    \includegraphics[width=0.6\textwidth]{figures/fig_1.png}
    \caption[Fig 1]{Figure description \label{fig_1}}
  \end{center}
  \end{figure}
%A much longer \LaTeXe{} example was written by Gil~\cite{Gil:02}.
In the example in Figure 1, hash \texttt{62f51643c1} was produced on
count $510144806912$ and hash \texttt{c43d862d88} was produced on
count $510146904064$. Real time passed between count $510144806912$
and count $510146904064$.

\subsection{Timestamp for Events}

This sequence of hashes can also be used to record that some piece of data was created before a particular hash index was generated.  Using a `combine` function to combine the piece of data with the current hash at the current index. The `data` can simply be a cryptographically unique hash of arbitrary event data. The combine function can be a simple append of data, or any operation that is collision resistant.\\

Arithmetic operations like addition, multiplication etc... wouldn’t work because an attacker could have precomputed a separate sequence in parallel, and could join the two by inserting a piece of data that would add up to the starting value of the parallel sequence. Append would force the attacker to try to create a collision between a hash, and the data they are trying to append.\\


\noindent For example:\\\\\noindent
\texttt{
sha256(\char`\"any starting value\char`\") $\rightarrow$ hash1,
(n\_count $=~1$)1\\
\ldots\\
sha256(hash199) $\rightarrow$ hash200, (n\_count $=~200$)\\
\ldots\\
sha256(hash299) $\rightarrow$ hash300, (n\_count $=~300$)\\
}

\noindent Some external event occurs, like a photograph was taken, or
any arbitrary digital data was created:\\\\\noindent
\texttt{
  sha256(hash334) $\rightarrow$ hash335, (n\_count $=~335$), photograph\_sha256\\
  sha256(append(hash335, photograph\_sha256) $\rightarrow$ hash336,
  (n\_count $=~336$)\\
  \ldots\\
  sha256(hash399) $\rightarrow$ hash400, (n\_count $=~400$)\\
}

\texttt{Hash336} is computed from the appended binary data of
\texttt{hash335} and the \texttt{sha256} of the photograph. The index,
and the \texttt{sha256} of the photograph are recorded as part of the
sequence output. So anyone verifying this sequence can then recreate
this change to the sequence. The verifying can still be done in
parallel:\\\\\noindent
\texttt{
  sha256(hash299) $\rightarrow$ hash300, (n\_count $=~300$)\\
  sha256(hash334) $\rightarrow$ hash335, (n\_count $=~335$), photograph\_sha256\\
}\\\noindent
And\\\\\noindent
\texttt{
  sha256(append(hash335, photograph\_sha256) $\rightarrow$ hash336,
  (n\_count $=~336$)\\
  sha256(hash399) $\rightarrow$ hash400, (n\_count $=~400$)\\
}

Because the initial process is still sequential, we can then tell that things entered into the sequence must have occurred sometime before the future hashed value was computed.\\\\\noindent
\texttt{
sha256(hash334) $\rightarrow$ hash335, (n\_count $=~335$), photograph1\_sha256\\
sha256(append(hash335, photograph\_sha256) $\rightarrow$ hash336,
(n\_count $=~336$)\\
\ldots\\
sha256(hash599) $\rightarrow$ hash600, (n\_count $=~600$), photograph2\_sha256
sha256(append(hash600, photograph2\_sha256) $\rightarrow$ hash601,
(n\_count $=~601$)\\
}

So \texttt{photograph2} was created before \texttt{hash601}, and
\texttt{photograph1} was created before \texttt{hash336}. Inserting this extra data into the sequence of hashes results in an unpredictable change to all subsequent values in the sequence. So it would be impossible to precompute any future sequences based on prior knowledge of what data will be mixed into the sequence.\\

The sequence only needs to mix and publish a hash of the event data into the event sequence. The mapping of the hash to event data can be stored outside of the sequence, and the event data can contain other metadata within itself, such as real time stamps and connection IPs.\\

\begin{figure}
  \begin{center}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/fig_2.png}
    \caption[Fig 2]{Figure description \label{fig_2}}
  \end{center}
  \end{figure}

  In the example in Figure 2, input \texttt{cfd40df8\ldots} was inserted into the Proof of History sequence. The count at which it was inserted is $510145855488$ and the state at which it was inserted it is \texttt{3d039eef3}.\\

Every node observing this sequence can determine the order at which all events have been inserted. Generating a reverse order would require an attacker to start the malicious sequence after the second event. This delay would allow any non malicious peer to peer nodes to communicate about the original order.\\

\subsection{Timestamp for Events}
\subsection{Verification}
The sequence can be verified as correct in a multi core computer in less time than it took to generated it 

\noindent For example: \\\\\noindent

Core1:
sha256(\char`\"any random starting value\char`\") $\rightarrow$ hash1, 1
...
sha256(hash199) $\rightarrow$ hash200, 200

Core2:
sha256(hash199) $\rightarrow$ hash200, 200
...
sha256(hash299) $\rightarrow$ hash300, 300


So given some number of cores, like a modern GPU with 4000 cores, the verifier can split up the sequence of hashes and their indexes into 4000 slices, and in parallel make sure that each slice is correct from the starting hash to the last hash in the slice.  So if the expected time to produce the sequence is going to be 

<number of hashes>/<hashes per second for 1 core>

the expected time to verify that the sequence is correct is going to be 

<number of hashes>/(<hashes per second per core> * <number of cores available to verify>)

\begin{figure}
  \begin{center}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/fig_3.png}
    \caption[Figure 3]{Verification using multiple cores\label{fig_3}}
  \end{center}
  \end{figure}

In the example in Figure 3, each core is able to verify each slice of the sequence in parallel.  Since all input strings are recorded into the output, with the counter and state that they are appended to, the verifiers can replicate each slice in parallel.
\subsection{Verification}

\subsection{Horizontal Scaling}
It’s possible to synchronize multiple Proof of History generators by mixing the sequence state from each generator to each other generator, and thus achieve horizontal scaling of the Proof of History generator.

Generator A
Hash1a
Hash2a
Hash3a hash(Hash2b, hash1a)
Hash4a

Generator B
Hash1b
Hash2b
Hash3b hash(Hash2a, hash1b)
Hash4b

GeneratorA receives a data packet from Generator B, which contains the last state from Generator B, and the last state generator b observed from GeneratorA.  The next state hash in Generator A then depends on the state from Generator B, so we can derive that hash2b happened sometime before hash4a.  This property can be transitive, so if three generators are synchronized through a single common generator A ⇔ B ⇔ C, we can trace the dependency between A and C even though they were not synchronized directly.

By periodically synchronizing the generators, each generator can then handle a portion of external traffic, thus the overall system can handle a larger amount of events to track at the cost of the accuracy due to network latencies between the generators.

Having multiple generators may make deployment more resistant to attacks.  One generator could be high bandwidth, and receive many events to mix into its sequence, another generator could be high speed low bandwidth that periodically mixes with the high bandwidth generator.

The high speed sequence would create a secondary sequence of data that an attacker would have to reverse.

\begin{figure}
  \begin{center}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/fig_4.png}
    \caption[Fig 4]{Two generators synchronizing\label{fig_4}}
  \end{center}
  \end{figure}

In the figure above, the two generators insert each other’s output state and record the operation.  

The synchronization is transitive.  A ⇔ B ⇔ C  There is a provable order of events between A and C through B.  Two generators can double bandwidth at the cost of availability.  10x1gbps connections with availability of 0.999 would have 0.999^10 -> 0.99 availability.
\subsection{Horizontal Scaling}
\subsection{Consistency}
Users can enforce consistency of the generated sequence and make it resilient to attacks by inserting the last observed output of the sequence they consider valid into their input.

hash9a
Event1, hash10a
Event2, hash20a
Event3, hash30a

If the events were all available to insert at the same time, or the service has a hidden clock that's slightly faster, the service could produce a second hidden sequence with the events in reverse order.  

hash9a
Event3, hash10b
Event2, hash20b
Event1, hash30b

Both sequences start at hash9a, so they are equal in length.  But as clients of this service, we want only a single valid sequence to exist. 

To prevent this attack, each client generated Event should contain within itself the latest hash that the client observed from what it considers to be a valid sequence.  So when a client creates the “Event1” hash, they should append the last hash they have observed.

Hash5a
Event1 = hash(append(event1 data, hash5a))
Event1, hash10a
hash15a
Event2 = hash(append(event2 data, hash15a))
Event2, hash20a
hash25a
Event3 = hash(append(event3 data, hash25a))
Event3, hash30a

When the sequence is published, Event3 would be referencing hash25a, and if it’s not in the sequence prior to this Event, the consumers of the sequence know that it’s an invalid sequence.  The partial reordering attack would then be limited to the number of hashes produced while the client has observed an event and when the event was entered.  Clients can then write software that doesn’t assume the order is correct for the short period of hashes between the last observed and inserted hash.

To prevent a malicious clock service from rewriting the client Event hashes, the clients can submit a signature of the event data and the last observed hash instead of just a hash

Event3 = sign(append(event3 data, hash25a), client Private Key)
Event3, hash30a

The mapping from the event hash or signature to event data and client’s public key can be published in a separate database for all the clients of the service to verify.
Verify:
	(Public Key, hash25a, event3 data) <- lookup Event3 
      Verify Event3 signature with Public Key
      Verify hash25a exists prior to hash31a in the sequence

\begin{figure}
  \begin{center}
    \centering
    \includegraphics[width=0.6\textwidth]{figures/fig_1.png}
    \caption[Fig 5]{Input with a back reference.\label{fig_1}}
  \end{center}
  \end{figure}

The above example, user supplied input is dependent on hash 0xdeadbeef existing in the generated sequence sometime before it’s inserted.
\subsection{Consistency}
\subsection{Advantages}

Proof of history provides some protection against long range attacks.  A malicious user that gains access to old private keys would have to recreate a historical record that takes as much time as the original one they are trying to forge.  This would require access to a faster processor than the network is currently using, otherwise the attacker would never catch up in history length.

Additionally, a single source of time allows for construction of a simper Proof of Replication (more on that later).  Since all the participants in the network can rely on a single historical record of events.  

PoRep and PoH together provide a defense of both space and time against a forged ledger.

\subsection{Advantages}
\section{Results}\label{results}
In this section we describe the results.

\section{Conclusions}\label{conclusions}
We worked hard, and achieved very little.

\bibliographystyle{abbrv}
\bibliography{simple}

\end{document}
This is never printed
